{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <font size='6' font-weight='bold'> DataBase Downloader </font> </center>  \n",
    "<center> <i> Projet Navee</i> </center>\n",
    "<center> <i> Tony </i> </center>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:**  \n",
    "Since it's not suited to always have to redownload the whole training set each time we want to make tests on it, we want to implement a quick and simple algorithm to download a certain number of pictures in the `data_training` folder.  \n",
    "\n",
    "Those files are going to be named accordingly to their corresponding ID in the database (i.e. 19388.png).  \n",
    "\n",
    "What comes next is that we'll have to create class that inherits from *DataGenerator* (from Data_Gen.py) and we'll change its behavious so much so that everything works the same way (for instance how we can seek particular labels in the database) but instead of having to download the files, it will directly look into the `data_training` directory.\n",
    "\n",
    "However, since it's not practical to download the whole database on our computers, the moment we'll generate the `data_training` folder, we'll also create a `database_BAM_training.sqlite` file that only contains the instances that were previously retrieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paramaters:**  \n",
    "Please set `nb_images` to be equal to the number of images to include in the training set.  \n",
    "Make sure that `image_size` must be the same in all different algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_images = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (224,224) # resize parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import urllib # to download images from the web\n",
    "from shutil import copyfile # to create a copy of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change to main file tree level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Tony/Desktop/projet-navee/data'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Tony/Desktop/projet-navee'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from data import * # imports data.py\n",
    "from Data_Gen import * # imports Data_Gen.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = 'data/database_BAM.sqlite'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the database has been correctly imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found ü§ôüèª\n"
     ]
    }
   ],
   "source": [
    "assert os.path.exists(db_path), \"Database not found üëéüèª\\n\\\n",
    "    Please check that you've successfully copied the database in the data\\\n",
    "    directory after having cloned the project ‚ÄºÔ∏è\"\n",
    "print ('Dataset found ü§ôüèª')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = data_base(db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`db` is a custom object defined in `data.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs = db.get_images_(nb_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {i:db.get_label(i) for i in IDs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`labels` has the shape:  \n",
    "{4288:\n",
    "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[159078039, 171966483, 143825721, 117440577, 224022099]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_IDs(db, list_IDs, skip_errors=False):\n",
    "    '''Download all images with the corresponding IDs from the specified database.\n",
    "    \n",
    "    Inputs:\n",
    "        - db = data_base object\n",
    "        - list_IDs = list of IDs\n",
    "    Output:\n",
    "        - list_errors = list of the images's ID that couldn't be downloaded.\n",
    "    '''\n",
    "    \n",
    "    list_errors = []\n",
    "    \n",
    "    for ID in list_IDs:\n",
    "        url = db.get_image(ID)\n",
    "    \n",
    "        try:\n",
    "            reponse = requests.get(url)\n",
    "            img = Image.open(bio(reponse.content))\n",
    "            \n",
    "            # NB: All images from the BAM dataset are .jpg.\n",
    "            filePath = os.path.join('data_training', str(ID)+'.jpg')\n",
    "            urllib.urlretrieve(url, filePath)\n",
    "        \n",
    "        except:\n",
    "            print(f'Couldn\\'t download image from {url} which corresponds to ID #{ID}.\\n')\n",
    "            \n",
    "            list_errors.append(ID)\n",
    "            \n",
    "            if not(skip_errors):\n",
    "                print('Download interrupted.')\n",
    "                return list_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset_db(db_path, list_IDs):\n",
    "    '''Create a copy of the given database in data_training and only keeps the instances\n",
    "    that have their ID in list_IDs.\n",
    "    \n",
    "    Inputs:\n",
    "        - db = data_base object\n",
    "        - list_IDs = list of IDs\n",
    "    Output:\n",
    "        - None.\n",
    "    '''\n",
    "    \n",
    "    db_path_training = os.path.join('data_training', 'database_BAM_training.sqlite')\n",
    "    \n",
    "    # First, we create a copy of the database.\n",
    "    copyfile(db_path, db_path_training)\n",
    "    \n",
    "    # Then we "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
